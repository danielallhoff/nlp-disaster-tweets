{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a repository for the Kaggle Challenge in \"Natural Language Processing with Disaster Tweets\". It consists of the prediction if the tweet mentions a real disaster or not.\n",
        "\n",
        "Challenge link: https://www.kaggle.com/competitions/nlp-getting-started/data"
      ],
      "metadata": {
        "id": "CCasa_7mZmDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install scikit-learn pandas re tqdm numpy\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device in use: {device}\")\n",
        "import os\n",
        "import sklearn\n",
        "import pandas\n",
        "import re\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import torch.utils.data as data_utils\n",
        "from tqdm import tqdm\n",
        "!pip install boto3 sentencepiece sacremoses transformers alive-progress\n",
        "from alive_progress import alive_bar\n",
        "import sys\n",
        "sys.stdout.isatty()\n",
        "import boto3\n",
        "import requests\n",
        "import regex\n",
        "import sentencepiece\n",
        "import sacremoses\n",
        "import transformers\n",
        "import random\n",
        "RANDOM_SEED=0\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "MODEL_CONFIG=\"bert-base-cased\""
      ],
      "metadata": {
        "id": "ucYmDJTCs5GF",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to add input from kaggle, use:"
      ],
      "metadata": {
        "id": "C3_ZDCUzVVLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "train_path=\"/kaggle/input/nlp-data/train.csv\"\n",
        "test_path=\"/kaggle/input/nlp-data/test.csv\"\n",
        "submission_path = \"/kaggle/input/nlp-data/sample_submission.csv\"\n"
      ],
      "metadata": {
        "id": "Y_JJrxHqgk5C",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenever the jupyter code is used on Google Collab, its possible to load the data from Google Drive with:"
      ],
      "metadata": {
        "id": "3nCHdkB3VWUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "## Mount google drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcSiLkrbs2I0",
        "outputId": "bff5de10-b4aa-4a3c-b4b8-a1a22f8f6afa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip the data!"
      ],
      "metadata": {
        "id": "nzBaTtvEZmDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Projects/Kaggle-Disaster-Tweets/data/nlp-getting-started.zip\n",
        "train_path=\"train.csv\"\n",
        "test_path=\"test.csv\"\n",
        "submission_path = \"sample_submission.csv\"\n"
      ],
      "metadata": {
        "id": "AtiX5C1OtxeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "IS0865GGP38-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data (training set and testing set) is loaded via a pandas."
      ],
      "metadata": {
        "id": "fIyJLF-UZmDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "df_train = pandas.read_csv(train_path)\n",
        "df_test = pandas.read_csv(test_path)\n",
        "df_test[\"target\"] = 0\n",
        "print(f\"Number of samples for training: {len(df_train)}\")\n",
        "print(f\"Number of samples for testing: {len(df_test)}\")\n",
        "print(\"Training data structure:\")\n",
        "print(df_train.keys())\n",
        "print(df_train.head())"
      ],
      "metadata": {
        "id": "VtpNWJ8rP5tz",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT prompting"
      ],
      "metadata": {
        "id": "WYnUMwz8MwHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because this problem is a NLP classification problem, we can make use of ChatGPT in order to make the classification. In order to do so, we require to make specific prompts and give context to ChatGPT for identifying if the text talks about a disaster or not."
      ],
      "metadata": {
        "id": "vdH4Bd_UZmDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal classification prompt\n",
        "prompt = \"You are a tweet analyst in order to monitor possible emergencies is posted online like accidents (car accidents, airplane accidents, train wrecks or any type of accident), natural disasters (for example: earthquakes, typhoon, tsunamis, storm damage, fire...etc), crimes (like homicides, killings, bombing, terrorism, casualties), war, scandals....etc.  It’s not always clear whether a tweet´s words are actually referring to a disaster that happened or is happening. ANSWER ONLY WITH ONE INT VALUE: 1 (if the tweet speaks about a disaster or emergency) OR 0 (if not)!!!!!. DO NOT ANSWER WITH MORE THAN ONE INT VALUE!!!! TEXT: {query} YOUR RESPONSE: \"\n"
      ],
      "metadata": {
        "id": "DxDU9deUpjSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Few shot learning prompt\n",
        "prompt = \"You are a tweet analyst in order to monitor possible emergencies is posted online like fire, car or airplane accidents, earthquakes, tsunamis, homicides, bombing, war, storm damage....etc.  It’s not always clear whether a tweet´s words are actually referring to a disaster that happened or is happening. ANSWER ONLY WITH ONE INT VALUE: 1 (if the tweet speaks about a disaster) OR 0 (if not)!!!!!. DO NOT ANSWER WITH MORE THAN ONE INT VALUE!!!! TEXT: On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE. DISASTER: 0. TEXT: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all. DISASTER:1. TEXT: I'm on top of the hill and I can see a fire in the woods... DISASTER: 1 TEXT: Jays rocking #MLB @JoeyBats19 just bombed one out of Rogers Centre. Play-offs r ahead for The #BlueJays - Bell Moseby and Barfield r back! DISASTER: 0 TEXT: {query} DISASTER: \"\n"
      ],
      "metadata": {
        "id": "N-uQzK14NEAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use openai library and pass key for making calls to ChatGPT with a specific prompt."
      ],
      "metadata": {
        "id": "DdttqQ9wZmDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai cohere tiktoken\n",
        "import openai\n",
        "import csv\n",
        "openai.api_key = \"KEY\"  # https://platform.openai.com/account/api-keys\n",
        "\n",
        "\n",
        "fieldnames = [\"id\",\"text\", \"pred\"]\n",
        "\n",
        "while True:\n",
        "  csv_filename = 'drive/MyDrive/chat_gpt_predictions.csv'\n",
        "  predictions = []\n",
        "  if os.path.exists(csv_filename):\n",
        "      predictions = pandas.read_csv(csv_filename, index_col=0)\n",
        "      predictions = predictions.index\n",
        "  else:\n",
        "      with open(csv_filename, 'w') as csvfile:\n",
        "        fieldnames = [\"id\",\"text\", \"pred\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "  try:\n",
        "    with open(csv_filename, 'a') as csvfile:\n",
        "        fieldnames = [\"id\",\"text\", \"pred\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        with alive_bar(int(len(df_test)-len(predictions)), force_tty=True) as bar:\n",
        "          for index, row in df_test.iterrows():\n",
        "              bar()\n",
        "              index = df_test.loc[index,\"id\"]\n",
        "              if index in predictions:\n",
        "                continue\n",
        "              content = prompt.format(query=row[\"text\"])\n",
        "\n",
        "              messages = [{\"role\": \"system\",\n",
        "                  \"content\": \"You are an useful tweet analysist.\"}, {\"role\": \"user\", \"content\": content}]\n",
        "\n",
        "              response = openai.ChatCompletion.create(\n",
        "                  model=\"gpt-3.5-turbo\", messages=messages, max_tokens=1)  #  Max tokens to 1 for just one token response\n",
        "\n",
        "              response_content = response.choices[0].message.content\n",
        "              writer.writerow({\"id\":index, \"text\":row[\"text\"], \"pred\":response_content})\n",
        "          break\n",
        "  except Exception as exception:\n",
        "    print(exception)\n"
      ],
      "metadata": {
        "id": "2aU5U897MzOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Submission\n",
        "chat_gpt_preds = pandas.read_csv(\"chat_gpt_predictions.csv\", index_col=0)\n",
        "df_submission = pandas.read_csv(submission_path, index_col=0)\n",
        "df_submission.loc[chat_gpt_preds.index, \"target\"] = chat_gpt_preds[\"pred\"].apply(float).apply(int)\n",
        "df_submission.to_csv(\"submission.csv\")"
      ],
      "metadata": {
        "id": "LjkCrpy9w5rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Test with google bard, another LLM or finetune a LLM with Lora or soft prompting"
      ],
      "metadata": {
        "id": "PcTivmejzIlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hGLtdn-EtbYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its important to shuffle the data so that the training algorithm does not see consecutive similar data:"
      ],
      "metadata": {
        "id": "gcBtOsyMZmDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = shuffle(df_train, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "NAPYtf47dctf",
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data augmentation"
      ],
      "metadata": {
        "id": "b59rZtNvZ8qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to add more variety to the data and avoid overfitting and more generalization, its possible to apply data augmentation. In this case, because we are handling text, the NLPAug library is used. With this library, multiple operations can be applied over text: Translation to another language and back to the original language, replace the text with synonims or antonyms, add lexical or gramatical errors...etc."
      ],
      "metadata": {
        "id": "-hrfuoilZmDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug\n",
        "import nlpaug.flow as naf\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "flow = naf.Sequential([\n",
        "    naw.BackTranslationAug(device=\"cuda\"),\n",
        "    naw.SynonymAug(aug_p=0.3)\n",
        "])\n",
        "\n",
        "print(f\"Number of samples for training before aug: {len(df_train)}\")\n",
        "percent_samples_aug = 0.2\n",
        "print(int(len(df_train) * percent_samples_aug))\n",
        "df_aug = df_train.iloc[0:int(len(df_train) * percent_samples_aug), :].copy()\n",
        "for text_idx in tqdm(range(len(df_aug[\"text\"]))):\n",
        "  text = df_aug.iloc[text_idx][\"text\"]\n",
        "  aug_text = flow.augment(text)\n",
        "  df_aug.iloc[text_idx][\"text\"] = aug_text\n",
        "\n",
        "print(f\"Number of samples for training after aug: {len(df_train)}\")\n",
        "\n",
        "df_aug.to_csv(\"train_aug.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "3mg3JL7JZ4cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_aug = pandas.read_csv(\"train_aug.csv\")"
      ],
      "metadata": {
        "id": "QDoWHbZ4xf3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pandas.concat([df_train, df_aug], ignore_index=True)\n",
        "df_train = shuffle(df_train, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "c01dYG4NcjPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-made preprocessing"
      ],
      "metadata": {
        "id": "yEMN0hDG_LHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IN PROGRESS**"
      ],
      "metadata": {
        "id": "pBALf1nOZmD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Data preprocessing\n",
        "def df_text_preprocessing(df):\n",
        "  sentences = [re.sub(\"[@#'.,!?-]\", '', text.lower()) for text in df[\"text\"]]\n",
        "  print(sentences[0:5])\n",
        "  words = [list(set(\" \".join(sentence))) for sentence in sentences]\n",
        "  df[\"words\"] = words\n",
        "  return df"
      ],
      "metadata": {
        "id": "Yn9hbZU_sjgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_text_preprocessing(df_train)\n",
        "print(df_train[\"words\"][2])\n",
        "print(df_train[\"text\"][2])"
      ],
      "metadata": {
        "id": "U-SjyQsq4EPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing BERT with Hugginface\n",
        "\n"
      ],
      "metadata": {
        "id": "MZ5Y4Ths_Cfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One option for doing NLP Classification (or any general AI task), is to finetune a large and generic model to a specific task like this. BERT is one of these large models. It was as \"Pre-training of Deep Bidirectional Transformers for Language Understanding\". On one side, its possible to encode the texts and finetune these encodings with Machine Learning Algorithms. On the other side, you can finetune the whole Transformer with the specific data."
      ],
      "metadata": {
        "id": "VgFZChs9ZmD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer"
      ],
      "metadata": {
        "id": "sM_JPxRfA0-e",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_tokenize_bert(texts, tokenizer, max_length=None):\n",
        "  if max_length is None:\n",
        "    max_length_info = max([len(str(text)) for text in texts])\n",
        "    print(f\"Max length info of: {max_length_info}\")\n",
        "    max_length = max_length_info\n",
        "  attention_mask = [[1 if idx < len(str(text)) else 0 for idx in range(max_length)] for text in texts]\n",
        "  tokens =[]\n",
        "  attention_mask = []\n",
        "  # TODO: use batch_encode_plus for faster extraction\n",
        "  for text in texts:\n",
        "    encoding = tokenizer.encode_plus(str(text), add_special_tokens=True,max_length=max_length,padding='max_length')\n",
        "    tokens.append(encoding.input_ids)\n",
        "    attention_mask.append(encoding.attention_mask)\n",
        "\n",
        "  return np.int32(tokens), np.array(attention_mask, dtype=bool), max_length"
      ],
      "metadata": {
        "id": "PmaSQTuK_9qY",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_predict_encodings(tokens, attention_mask, targets, model, batch_size=8, output_layer=\"pooler_output\"):\n",
        "  tokens_tensor = data_utils.TensorDataset(torch.tensor(tokens), torch.ByteTensor(attention_mask),torch.IntTensor(targets))\n",
        "  predict_loader = data_utils.DataLoader(dataset = tokens_tensor, batch_size = batch_size, shuffle = False)  # For preprocessing\n",
        "  encodings = []\n",
        "  model = model.to(device)\n",
        "  with alive_bar(int(len(tokens)/batch_size)) as bar:\n",
        "    with torch.no_grad():\n",
        "      for texts, attention_mask, _ in predict_loader:\n",
        "        texts = texts.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        encodings_batch = model(texts, attention_mask)\n",
        "        encodings_batch = getattr(encodings_batch, output_layer).cpu() # Get encodings\n",
        "        encodings.extend(np.float32(encodings_batch))\n",
        "        bar()\n",
        "\n",
        "\n",
        "  return np.float32(encodings)"
      ],
      "metadata": {
        "id": "XShQ0xWO_HLL",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_bert_encodings(df, key=\"text\", max_length=None, model_config=\"bert-base-uncased\", batch_size=256, output_layer=\"pooler_output\"):\n",
        "  tokenizer = BertTokenizer.from_pretrained(model_config)\n",
        "  model = BertModel.from_pretrained(model_config)\n",
        "  tokens, attention_mask, max_length = df_tokenize_bert(df[key], tokenizer, max_length=max_length)\n",
        "  print(f\"Length tokens in use {len(tokens[0])}\")\n",
        "  encodings = df_predict_encodings(tokens, attention_mask, df[\"target\"], model, batch_size, output_layer)\n",
        "  print(f\"Number of encondings: {len(encodings)}\")\n",
        "  print(f\"Encondings shape: {encodings.shape}\")\n",
        "  return encodings, max_length"
      ],
      "metadata": {
        "id": "9WSdwImUxXOs",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the training encodings!"
      ],
      "metadata": {
        "id": "nvYcWa_GZmD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert encodings\n",
        "X_train, max_length = load_bert_encodings(df_train, key=\"text\", max_length=300, model_config=MODEL_CONFIG, batch_size=256, output_layer=\"pooler_output\")\n",
        "Y_train = df_train[\"target\"]"
      ],
      "metadata": {
        "id": "ixyqtc0kqabO",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful feature from the training data is the keyword. This keyword can also be encoded and used."
      ],
      "metadata": {
        "id": "yEeJB6I6ZmEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_keyword, max_length = load_bert_encodings(df_train, key=\"keyword\", max_length=300, model_config=MODEL_CONFIG, batch_size=256, output_layer=\"pooler_output\")"
      ],
      "metadata": {
        "id": "8W9IwxnJyPgO",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, _ = load_bert_encodings(df_test, max_length=max_length, model_config=MODEL_CONFIG, batch_size=32, output_layer=\"pooler_output\")\n"
      ],
      "metadata": {
        "id": "MIl-UF_Hnzm5",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_keyword, _ = load_bert_encodings(df_test, key=\"keyword\",max_length=max_length, model_config=MODEL_CONFIG, batch_size=32, output_layer=\"pooler_output\")"
      ],
      "metadata": {
        "id": "yqIH0O7DynTJ",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_features = X_train.copy()\n",
        "X_train_keyword_features = X_train_keyword.copy()\n",
        "X_test_features = X_test.copy()\n",
        "X_test_keyword_features = X_test_keyword.copy()\n",
        "Y_train_features = Y_train.copy()"
      ],
      "metadata": {
        "id": "8Kz0lvSkgk5G",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Join text and keyword features"
      ],
      "metadata": {
        "id": "_7NSMPwjyxey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join these features into one encoding!"
      ],
      "metadata": {
        "id": "Z2UfrE-yZmEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train shape before: {X_train_features.shape}\")\n",
        "X_train_features = np.concatenate((X_train_features, X_train_keyword_features), axis=-1)\n",
        "X_test_features = np.concatenate((X_test_features, X_test_keyword_features), axis=-1)\n",
        "print(f\"X_train shape after: {X_train_features.shape}\")"
      ],
      "metadata": {
        "id": "OtMMXXGMpTzo",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and val split"
      ],
      "metadata": {
        "id": "n3kQZtFXyN8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In every machine learning problem, data is splitted into a training set and testing set. Moreover, it can exist a specific validation set which is used as a previous step to evaluate the testing set. For example, the validation set in deep learning can be used for early stopping (stop training when the model does not improve over the validation set)."
      ],
      "metadata": {
        "id": "Bo9jEAlWZmEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "NUM_SAMPLES_VALIDATION = 500\n",
        "X_val_split = X_train_features[0:NUM_SAMPLES_VALIDATION]\n",
        "Y_val_split = Y_train_features[0:NUM_SAMPLES_VALIDATION]\n",
        "X_train_split = X_train_features[NUM_SAMPLES_VALIDATION:]\n",
        "Y_train_split = Y_train_features[NUM_SAMPLES_VALIDATION:]\n",
        "# X_train_split, X_val_split, Y_train_split, Y_val_split = train_test_split(X_train, Y_train, test_size=0.10, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "d5Nsu4LlyPyw",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning"
      ],
      "metadata": {
        "id": "O0FaJtSuPNxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE, RandomUnderSampler...etc"
      ],
      "metadata": {
        "id": "Vwt5F0tGZD-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SMOTE technique helps to augment synthetically the unrepresented class of the training set. The synthetic data is created by creating intermediate representations of the real existing data."
      ],
      "metadata": {
        "id": "0bI3DEI-ZmED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "sampling_strategy = \"over\"\n",
        "\n",
        "if sampling_strategy == \"over\":\n",
        "  pipeline = SMOTE(random_state=RANDOM_SEED)\n",
        "elif sampling_strategy == \"under\":\n",
        "  pipeline = RandomUnderSampler(sampling_strategy=\"majority\", random_state=RANDOM_SEED)\n",
        "elif sampling_strategy == \"both\":\n",
        "  over = SMOTE(sampling_strategy=0.85, random_state=RANDOM_SEED)\n",
        "  under = RandomUnderSampler(sampling_strategy=\"majority\", random_state=RANDOM_SEED)\n",
        "\n",
        "  steps = [('o', over), ('u', under)]\n",
        "  pipeline = Pipeline(steps=steps)\n",
        "\n",
        "X_train_split, Y_train_split = pipeline.fit_resample(X_train_split, Y_train_split)"
      ],
      "metadata": {
        "id": "5WGMBjFyZGjS",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "YTHcFadJ0D7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) can be useful to reduce the high dimensions of the features by keeping the most relevant ones (the features which has less correlation between the rest and more variety)."
      ],
      "metadata": {
        "id": "JAtRvLd6ZmED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=64)\n",
        "pca.fit(X_train_split)\n",
        "X_train_split = pca.transform(X_train_split)\n",
        "X_val_split = pca.transform(X_val_split)\n",
        "X_test_features = pca.transform(X_test_features)"
      ],
      "metadata": {
        "id": "4b3KZayk0GL9",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "TGk9QZJMwnws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (SVM) is one of the most used machine learning classification algorithms."
      ],
      "metadata": {
        "id": "f0NLS9tvZmEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC()\n",
        "clf.fit(X_train_split, Y_train_split)\n",
        "\n",
        "Y_pred = clf.predict(X_val_split)\n",
        "Y_test_pred = clf.predict(X_test_features)"
      ],
      "metadata": {
        "id": "DCTbT0-_PPuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN\n"
      ],
      "metadata": {
        "id": "bJ2oTh1Sn-bR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors (KNN) is a classification algorithm which checks the K most nearest neighbors of a specific data point in order to find the most probable class it represents."
      ],
      "metadata": {
        "id": "kyYR9an4ZmEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_classifier.fit(X_train_split, Y_train_split)\n",
        "\n",
        "Y_pred = knn_classifier.predict(X_val_split)\n",
        "Y_test_pred = knn_classifier.predict(X_test_features)"
      ],
      "metadata": {
        "id": "MMWNpRVAoC2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forests"
      ],
      "metadata": {
        "id": "XDMCMZLdYnta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests creates a big decission tree of the data automatically for classification."
      ],
      "metadata": {
        "id": "nK6Tks-JZmEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train_split, Y_train_split)\n",
        "\n",
        "Y_pred = clf.predict(X_val_split)\n",
        "Y_test_pred = clf.predict(X_test_features)"
      ],
      "metadata": {
        "id": "UM2Z2cUFYpJs",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost (Boosted trees)\n"
      ],
      "metadata": {
        "id": "rsRIP0A6nD8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In progress"
      ],
      "metadata": {
        "id": "sCIeu0o_qco4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "dmatrix_train = xgb.DMatrix(X_train_split, label=Y_train_split)\n",
        "dmatrix_val = xgb.DMatrix(X_val_split, label=Y_val_split)\n",
        "test =  xgb.DMatrix(X_test_features)\n",
        "val_xgb = [(dmatrix_val, \"eval\"), (dmatrix_train, \"train\")]\n",
        "params =  {\"booster\":\"gbtree\", \"max_depth\": 2, \"eta\": 0.3, \"objective\": \"binary:logistic\", \"nthread\":2}\n",
        "n_rounds = 10\n",
        "model = xgb.train(params, dmatrix_train, n_rounds, val_xgb)\n",
        "Y_test_pred = model.predict(test)"
      ],
      "metadata": {
        "id": "FHNjHnznnHux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Networks"
      ],
      "metadata": {
        "id": "VE9rj1dJtaEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLPClassifier\n",
        "\n"
      ],
      "metadata": {
        "id": "BeW__EVPKOZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Multi-Layer Perceptron (MLP) learns the best weights in order to fit the data with a loss function."
      ],
      "metadata": {
        "id": "Qj0iqIm4ZmEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clf = MLPClassifier(random_state=RANDOM_SEED, max_iter=1000, activation=\"logistic\",learning_rate=\"adaptive\", batch_size=128, early_stopping=True, verbose=True, n_iter_no_change=200)\n",
        "clf.fit(X_train_split, Y_train_split)\n",
        "Y_pred = clf.predict(X_val_split)\n",
        "Y_test_pred = clf.predict(X_test_features)"
      ],
      "metadata": {
        "id": "6pBkkqEyPNKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Finetuning"
      ],
      "metadata": {
        "id": "fXLdrr1wtd2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Dataset"
      ],
      "metadata": {
        "id": "0KdALxRlRRVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 200\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 1e-06"
      ],
      "metadata": {
        "id": "clKioR0xTEyi",
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len, extra_feature_location=False, extra_feature_keyword=False, class_weights=False, random_extra_feature_dropout=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.location = dataframe.location\n",
        "        self.keyword = dataframe.keyword\n",
        "        self.idxs = dataframe.index\n",
        "        self.targets = self.data.target\n",
        "        self.max_len = max_len\n",
        "        self.class_weights = class_weights\n",
        "        self.extra_feature_location = extra_feature_location\n",
        "        self.extra_feature_keyword = extra_feature_keyword\n",
        "        self.random_extra_feature_dropout = random_extra_feature_dropout\n",
        "        self.weights_per_class = compute_class_weight(class_weight=\"balanced\",y=self.targets, classes=np.unique(self.targets))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        idx = self.idxs[index]\n",
        "        text = str(self.text[idx])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        if self.extra_feature_keyword:\n",
        "            keyword = self.keyword[idx]\n",
        "            if self.random_extra_feature_dropout and random.uniform(0, 1) > 0.5:\n",
        "              keyword = \" \"\n",
        "            text += \" keyword=\" + str(keyword) + \" \"\n",
        "\n",
        "\n",
        "        if self.extra_feature_location:\n",
        "            location = self.location[idx]\n",
        "            if self.random_extra_feature_dropout and  random.uniform(0, 1) > 0.5:\n",
        "              keyword = \" \"\n",
        "            text += \" location=\" + str(location) + \" \"\n",
        "\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "        weight = torch.tensor(self.weights_per_class[self.targets[idx]], dtype=torch.float)\n",
        "        targets = torch.tensor([self.targets[idx]], dtype=torch.float)\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': targets,\n",
        "            'weight': weight\n",
        "        }"
      ],
      "metadata": {
        "id": "kqQovMQdRS0A",
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_CONFIG)\n",
        "new_df = df_train[['text', 'target', \"keyword\", \"location\"]].copy()\n",
        "NUM_SAMPLES_VALIDATION = 500\n",
        "EXTRA_FEATURE_LOCATION=False\n",
        "EXTRA_FEATURE_KEYWORD=True\n",
        "RANDOM_EXTRA_FEATURE_DROPOUT=True\n",
        "training_df = new_df[NUM_SAMPLES_VALIDATION:]\n",
        "validation_df = new_df[0: NUM_SAMPLES_VALIDATION]\n",
        "training_loader = CustomDataset(\n",
        "    training_df,\n",
        "    tokenizer,\n",
        "    max_len=MAX_LEN,\n",
        "    extra_feature_location=EXTRA_FEATURE_LOCATION,\n",
        "    extra_feature_keyword=EXTRA_FEATURE_KEYWORD,\n",
        "    random_extra_feature_dropout=RANDOM_EXTRA_FEATURE_DROPOUT\n",
        ")\n",
        "validation_loader = CustomDataset(\n",
        "    validation_df,\n",
        "    tokenizer,\n",
        "    max_len=MAX_LEN,\n",
        "    extra_feature_location=EXTRA_FEATURE_LOCATION,\n",
        "    extra_feature_keyword=EXTRA_FEATURE_KEYWORD,\n",
        "    random_extra_feature_dropout=RANDOM_EXTRA_FEATURE_DROPOUT\n",
        ")\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_loader, **train_params)\n",
        "validation_loader = DataLoader(validation_loader, **valid_params)\n"
      ],
      "metadata": {
        "id": "RQpzC6CCtZ6Q",
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition"
      ],
      "metadata": {
        "id": "v4e3aH5rT-GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = model\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768,64)\n",
        "        self.l4 = torch.nn.Linear(64,1)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(input_ids=ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output_3 = self.l3(output_2)\n",
        "        output = self.l4(output_3)\n",
        "        activation = torch.sigmoid(output)\n",
        "        return activation\n",
        "\n",
        "model = BERTClass(model=BertModel.from_pretrained(MODEL_CONFIG))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "UaF2SydmStG8",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "dpoFiG45T_wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets,weights=None):\n",
        "    losses = torch.nn.BCELoss(reduction='none')(outputs, targets, )\n",
        "    return torch.mean(weights*losses)\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n"
      ],
      "metadata": {
        "id": "T5t7aY6fUBBi",
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(data, model):\n",
        "    ids = data['ids'].to(device, dtype = torch.long)\n",
        "    mask = data['mask'].to(device, dtype = torch.long)\n",
        "    token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "    targets = data['targets'].to(device, dtype = torch.float)\n",
        "    weights = data['weight'].to(device, dtype = torch.float)\n",
        "    outputs = model(ids, mask, token_type_ids)\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(outputs, targets,weights)\n",
        "    return loss\n",
        "\n",
        "def train(epochs, model, training_loader, validation_loader, early_stopping=False, n_iter_no_change=5):\n",
        "    MIN_VAL_LOSS = 100000000\n",
        "    best_model = model\n",
        "    n_iter_no_change_count = 0\n",
        "    for epoch in range(epochs):\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      num_iterations = 1\n",
        "      for _,data in enumerate(training_loader, 0):\n",
        "          optimizer.zero_grad()\n",
        "          loss = forward(data, model)\n",
        "          total_loss += loss.item()\n",
        "          print(f'Epoch: {epoch}, Loss:  {total_loss/num_iterations}')\n",
        "          num_iterations += 1\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "      if early_stopping:\n",
        "        total_val_loss = 0\n",
        "        num_val_iterations = 1\n",
        "        print(\"Validating...\")\n",
        "        for _, data in enumerate(validation_loader, 0):\n",
        "            val_loss = forward(data, model)\n",
        "            total_val_loss += val_loss.item()\n",
        "            num_val_iterations += 1\n",
        "        val_loss = total_val_loss/num_val_iterations\n",
        "        print(f\"Val_loss: {val_loss}, MIN_VAL_LOSS: {MIN_VAL_LOSS}\")\n",
        "        if val_loss < MIN_VAL_LOSS:\n",
        "            MIN_VAL_LOSS = val_loss\n",
        "            n_iter_no_change_count = 0\n",
        "            print(f\"Validation loss has improved to {val_loss}!\")\n",
        "            torch.save(model.state_dict(), \"bert_finetuned\")\n",
        "            best_model = model\n",
        "        else:\n",
        "          n_iter_no_change_count += 1\n",
        "        print(f'Epoch: {epoch}, Loss:  {total_loss/num_iterations}, Validation loss: {val_loss}')\n",
        "        if n_iter_no_change_count >= n_iter_no_change:\n",
        "            return best_model\n",
        "    return best_model"
      ],
      "metadata": {
        "id": "LpelyYevUI85",
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _,data in enumerate(training_loader, 0):\n",
        "    pass"
      ],
      "metadata": {
        "trusted": true,
        "id": "4mzG4ci-ZmEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(EPOCHS, model, training_loader, validation_loader, early_stopping=True, n_iter_no_change=5)"
      ],
      "metadata": {
        "id": "LEDszHr7Wvca",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_loader = CustomDataset(\n",
        "    df_test, tokenizer, max_len=MAX_LEN,\n",
        "    extra_feature_location=EXTRA_FEATURE_LOCATION,\n",
        "    extra_feature_keyword=EXTRA_FEATURE_KEYWORD\n",
        ")\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "testing_loader = DataLoader(testing_loader, **test_params)"
      ],
      "metadata": {
        "id": "fMvv84NrXxEP",
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, testing_loader):\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_outputs.extend((outputs.cpu().detach().numpy() > 0.5)[:,0].astype(int).tolist() )\n",
        "    return fin_outputs"
      ],
      "metadata": {
        "id": "IefxnOPjXvJs",
        "trusted": true
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_test = predict(model, testing_loader)"
      ],
      "metadata": {
        "id": "GeD7aAMOYLoX",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "cMzOFQjQ2p5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "def eval_test(y_true, y_pred):\n",
        "  precission, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
        "  print(f\"Population {len(y_true)}\")\n",
        "  print(f\"F1-score\\t| Precission\\t| Recall\")\n",
        "  print(f\"{(fscore*100).round(2)}%\\t\\t| {(precission*100).round(2)}%\\t| {(recall*100).round(2)}%\")\n",
        "  return precission, recall, fscore"
      ],
      "metadata": {
        "id": "0JUpd9wy2pnD",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precission, recall, fscore = eval_test(Y_val_split.values, Y_pred)"
      ],
      "metadata": {
        "id": "PXUR_3CVyt-_",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "3uKckY5AW1Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(model, open(\"bert.sav\", 'wb'))\n",
        "# Reload clf\n",
        "# loaded_model = pickle.load(open(\"model.sav\", 'rb'))"
      ],
      "metadata": {
        "id": "q6TFsAN8W2jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit\n",
        "\n"
      ],
      "metadata": {
        "id": "SRZ4vZB8jp5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission = pandas.read_csv(submission_path)\n",
        "df_submission.loc[:,\"target\"] = Y_pred_test\n",
        "df_submission.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "PlHkTRjtj7b9",
        "trusted": true
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}
