{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device in use: {device}\")\n",
        "import sklearn\n",
        "import pandas\n",
        "import re\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import torch.utils.data as data_utils\n",
        "from tqdm import tqdm\n",
        "!pip install boto3 sentencepiece sacremoses transformers alive-progress\n",
        "from alive_progress import alive_bar\n",
        "import boto3\n",
        "import requests\n",
        "import regex\n",
        "import sentencepiece\n",
        "import sacremoses\n",
        "import transformers\n",
        "RANDOM_SEED=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucYmDJTCs5GF",
        "outputId": "8ae6cafb-2b78-47a4-d921-4b9ae3dcf6eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device in use: cuda:0\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.28.73-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alive-progress\n",
            "  Downloading alive_progress-3.1.4-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.32.0,>=1.31.73 (from boto3)\n",
            "  Downloading botocore-1.31.73-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3)\n",
            "  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting about-time==4.2.1 (from alive-progress)\n",
            "  Downloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
            "Collecting grapheme==0.6.0 (from alive-progress)\n",
            "  Downloading grapheme-0.6.0.tar.gz (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.73->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.73->boto3) (2.0.7)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Building wheels for collected packages: sacremoses, grapheme\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=c3479950c468061c65181ffe77a0ee10136e3ac4bd74e304b99f86cc560e414b\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210079 sha256=594afaffd45cc14e2629cbad09f102d35af6b47a9271b83f3c963880eb1148e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e1/49/37e6bde9886439057450c494a79b0bef8bbe897a54aebfc757\n",
            "Successfully built sacremoses grapheme\n",
            "Installing collected packages: sentencepiece, grapheme, safetensors, sacremoses, jmespath, about-time, huggingface-hub, botocore, alive-progress, tokenizers, s3transfer, transformers, boto3\n",
            "Successfully installed about-time-4.2.1 alive-progress-3.1.4 boto3-1.28.73 botocore-1.31.73 grapheme-0.6.0 huggingface-hub-0.17.3 jmespath-1.0.1 s3transfer-0.7.0 sacremoses-0.0.53 safetensors-0.4.0 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Mount google drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcSiLkrbs2I0",
        "outputId": "8c5ce5ed-44ef-4e8b-b28d-9693ac81f8f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Projects/Kaggle-Disaster-Tweets/data/nlp-getting-started.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtiX5C1OtxeJ",
        "outputId": "af6adf3b-0e8a-411c-b566-6392b468d61a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Projects/Kaggle-Disaster-Tweets/data/nlp-getting-started.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hGLtdn-EtbYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "IS0865GGP38-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df_train = pandas.read_csv(\"train.csv\")\n",
        "df_test = pandas.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Number of samples for training: {len(df_train)}\")\n",
        "print(f\"Number of samples for testing: {len(df_test)}\")\n",
        "print(\"Training data structure:\")\n",
        "print(df_train.keys())\n",
        "print(df_train.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtpNWJ8rP5tz",
        "outputId": "65ed4171-6a1c-4d4e-e205-3a2dcb7f4beb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples for training: 7613\n",
            "Number of samples for testing: 3263\n",
            "Training data structure:\n",
            "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n",
            "   id keyword location                                               text  \\\n",
            "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
            "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
            "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
            "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
            "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
            "\n",
            "   target  \n",
            "0       1  \n",
            "1       1  \n",
            "2       1  \n",
            "3       1  \n",
            "4       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data augmentation"
      ],
      "metadata": {
        "id": "b59rZtNvZ8qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In progress..."
      ],
      "metadata": {
        "id": "3mg3JL7JZ4cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-made preprocessing"
      ],
      "metadata": {
        "id": "yEMN0hDG_LHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Yn9hbZU_sjgf"
      },
      "outputs": [],
      "source": [
        "## Data preprocessing\n",
        "def df_text_preprocessing(df):\n",
        "  sentences = [re.sub(\"[@#'.,!?-]\", '', text.lower()) for text in df[\"text\"]]\n",
        "  print(sentences[0:5])\n",
        "  words = [list(set(\" \".join(sentence))) for sentence in sentences]\n",
        "  df[\"words\"] = words\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_text_preprocessing(df_train)\n",
        "print(df_train[\"words\"][2])\n",
        "print(df_train[\"text\"][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-SjyQsq4EPb",
        "outputId": "90f199d4-ef72-4803-c609-ec17da60659a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['our deeds are the reason of this earthquake may allah forgive us all', 'forest fire near la ronge sask canada', 'all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected', '13000 people receive wildfires evacuation orders in california ', 'just got sent this photo from ruby alaska as smoke from wildfires pours into a school ']\n",
            "['t', 's', 'y', 'b', 'p', 'f', 'e', 'o', 'l', 'c', 'd', 'g', 'h', 'r', 'a', 'n', 'i', ' ', 'u', 'x', 'k', 'v']\n",
            "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing BERT with Hugginface\n",
        "\n"
      ],
      "metadata": {
        "id": "MZ5Y4Ths_Cfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer"
      ],
      "metadata": {
        "id": "sM_JPxRfA0-e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_tokenize_bert(texts, tokenizer, max_length=None):\n",
        "  if max_length is None:\n",
        "    max_length = max([len(text) for text in texts])\n",
        "  tokens = [tokenizer.encode(re.sub(\"[@#'.,!?-]\", '', text.lower()), add_special_tokens=True, max_length=max_length, padding='max_length', truncation=True) for text in texts]\n",
        "\n",
        "  return np.int32(tokens)\n"
      ],
      "metadata": {
        "id": "PmaSQTuK_9qY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_predict_encodings(tokens, targets, model, batch_size=8, output_layer=\"pooler_output\"):\n",
        "  tokens_tensor = data_utils.TensorDataset(torch.tensor(tokens), torch.IntTensor(targets))\n",
        "  predict_loader = data_utils.DataLoader(dataset = tokens_tensor, batch_size = batch_size, shuffle = False)  # For preprocessing\n",
        "  encodings = []\n",
        "  model = model.to(device)\n",
        "  with alive_bar(len(tokens)) as bar:\n",
        "    with torch.no_grad():\n",
        "      for texts, _ in predict_loader:\n",
        "        texts = texts.to(device)\n",
        "        encodings_batch = model(texts)\n",
        "        encodings_batch = getattr(encodings_batch, output_layer).cpu() # Get encodings\n",
        "        encodings.extend(np.float32(encodings_batch))\n",
        "        bar()\n",
        "\n",
        "\n",
        "  return np.float32(encodings)"
      ],
      "metadata": {
        "id": "XShQ0xWO_HLL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_bert_encodings(df, max_length=None, batch_size=256, output_layer=\"pooler_output\"):\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  model = BertModel.from_pretrained('bert-base-uncased')\n",
        "  tokens = df_tokenize_bert(df_train[\"text\"], tokenizer, max_length=None)\n",
        "  encodings = df_predict_encodings(tokens, df[\"target\"], model, batch_size, output_layer)\n",
        "  print(f\"Number of encondings: {len(encodings)}\")\n",
        "  print(f\"Encondings shape: {encodings.shape}\")\n",
        "  return encodings\n"
      ],
      "metadata": {
        "id": "9WSdwImUxXOs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert encodings\n",
        "X_train = load_bert_encodings(df_train, max_length=None, batch_size=256, output_layer=\"pooler_output\")\n",
        "Y_train = df_train[\"target\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixyqtc0kqabO",
        "outputId": "daa1869e-44c4-4905-9d48-65026d5bd0a6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        0%|          | 0/30 [00:00<?, ?it/s]\n",
            "        3%|▎         | 1/30 [00:02<00:58,  2.01s/it]\n",
            "        7%|▋         | 2/30 [00:04<00:56,  2.00s/it]\n",
            "       10%|█         | 3/30 [00:06<00:54,  2.01s/it]\n",
            "       13%|█▎        | 4/30 [00:08<00:52,  2.01s/it]\n",
            "       17%|█▋        | 5/30 [00:10<00:50,  2.02s/it]\n",
            "       20%|██        | 6/30 [00:12<00:48,  2.02s/it]\n",
            "       23%|██▎       | 7/30 [00:14<00:46,  2.03s/it]\n",
            "       27%|██▋       | 8/30 [00:16<00:44,  2.03s/it]\n",
            "       30%|███       | 9/30 [00:18<00:42,  2.03s/it]\n",
            "        33%|███▎      | 10/30 [00:20<00:40,  2.03s/it]\n",
            "        37%|███▋      | 11/30 [00:22<00:38,  2.03s/it]\n",
            "        40%|████      | 12/30 [00:24<00:36,  2.03s/it]\n",
            "        43%|████▎     | 13/30 [00:26<00:34,  2.04s/it]\n",
            "        47%|████▋     | 14/30 [00:28<00:32,  2.04s/it]\n",
            "        50%|█████     | 15/30 [00:30<00:30,  2.05s/it]\n",
            "        53%|█████▎    | 16/30 [00:32<00:28,  2.05s/it]\n",
            "        57%|█████▋    | 17/30 [00:34<00:26,  2.05s/it]\n",
            "        60%|██████    | 18/30 [00:36<00:24,  2.05s/it]\n",
            "        63%|██████▎   | 19/30 [00:38<00:22,  2.05s/it]\n",
            "        67%|██████▋   | 20/30 [00:40<00:20,  2.05s/it]\n",
            "        70%|███████   | 21/30 [00:42<00:18,  2.05s/it]\n",
            "        73%|███████▎  | 22/30 [00:44<00:16,  2.06s/it]\n",
            "        77%|███████▋  | 23/30 [00:46<00:14,  2.06s/it]\n",
            "        80%|████████  | 24/30 [00:48<00:12,  2.06s/it]\n",
            "        83%|████████▎ | 25/30 [00:51<00:10,  2.07s/it]\n",
            "        87%|████████▋ | 26/30 [00:53<00:08,  2.07s/it]\n",
            "        90%|█████████ | 27/30 [00:55<00:06,  2.07s/it]\n",
            "        93%|█████████▎| 28/30 [00:57<00:04,  2.08s/it]\n",
            "        97%|█████████▋| 29/30 [00:59<00:02,  2.09s/it]\n",
            "       100%|██████████| 30/30 [01:00<00:00,  1.90s/it]\n",
            "       100%|██████████| 30/30 [01:00<00:00,  2.03s/it]\n",
            "on 30: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|▏⚠︎                                      | (!) 30/7613 [0%] in 1:00.9 (0.49/s) \n",
            "Number of encondings: 7613\n",
            "Encondings shape: (7613, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and val split"
      ],
      "metadata": {
        "id": "n3kQZtFXyN8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.10, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "d5Nsu4LlyPyw"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning"
      ],
      "metadata": {
        "id": "O0FaJtSuPNxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE"
      ],
      "metadata": {
        "id": "Vwt5F0tGZD-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In progress... SMOTE is an oversampling technique used to address data imbalance by generating synthetic samples from the underrepresented class."
      ],
      "metadata": {
        "id": "5WGMBjFyZGjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "YTHcFadJ0D7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=64)\n",
        "pca.fit(X_train)\n",
        "X_train = pca.transform(X_train)\n",
        "X_val = pca.transform(X_val)"
      ],
      "metadata": {
        "id": "4b3KZayk0GL9"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "TGk9QZJMwnws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC()\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = clf.predict(X_val)"
      ],
      "metadata": {
        "id": "DCTbT0-_PPuC"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forests"
      ],
      "metadata": {
        "id": "XDMCMZLdYnta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = clf.predict(X_val)"
      ],
      "metadata": {
        "id": "UM2Z2cUFYpJs"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Networks"
      ],
      "metadata": {
        "id": "VE9rj1dJtaEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Test basic MLP"
      ],
      "metadata": {
        "id": "6pBkkqEyPNKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "fXLdrr1wtd2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Training BERT, small transformers or LSTM...etc"
      ],
      "metadata": {
        "id": "RQpzC6CCtZ6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "cMzOFQjQ2p5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "def eval_test(y_true, y_pred):\n",
        "  precission, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
        "  print(f\"F1-score\\t| Precission\\t| Recall\")\n",
        "  print(f\"{(fscore*100).round(2)}%\\t\\t| {(precission*100).round(2)}%\\t| {(recall*100).round(2)}%\")\n",
        "  return precission, recall, fscore\n"
      ],
      "metadata": {
        "id": "0JUpd9wy2pnD"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precission, recall, fscore = eval_test(Y_val.values, Y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXUR_3CVyt-_",
        "outputId": "dee49f88-ed60-4c9a-a8b4-7ca9c7fb87e7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score\t| Precission\t| Recall\n",
            "61.38%\t\t| 69.05%\t| 55.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "3uKckY5AW1Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(pca, open(\"pca.sav\", 'wb'))\n",
        "# Reload clf\n",
        "# loaded_model = pickle.load(open(\"model.sav\", 'rb'))"
      ],
      "metadata": {
        "id": "q6TFsAN8W2jO"
      },
      "execution_count": 67,
      "outputs": []
    }
  ]
}